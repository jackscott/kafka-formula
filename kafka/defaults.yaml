
{% load_yaml as rawmap %}
Ubuntu:
  user: kafka
  service: kafka-broker
  prefix: /usr/local
  config_dir: /etc/kafka
  data_dir: /tmp/kafka-logs
  
  version: '0.8.2.2'
  scala_version: '2.10'
  
  # These are used to build the resources to download and isntall things.
  # Unless kafka changes the format or this mirror is no bueno, dont touch!
  name_template: 'kafka_%(scala_version)s-%(version)s'
  mirror_template: 'http://apache.mirrors.tds.net/kafka/%(version)s/%(name)s.tgz'
{% endload %}

{% load_yaml as config %}
port: 9092

broker:
  id: 42
  id.generation.enable: true

compression:
  # can also be gzip|snappy|lz4
  # producer will retain the codec set by the producer
  type: gzip

delete:
  topic.enable: false

advertised:
  host.name: ~
  listeners: ~
  port: ~

leader:
  imbalance.check.interval.seconds: 300
  imbalance.per.broker.percentage: 10

auto:
  create.topics.enable: True
  leader.rebalance.enable: True

log:
  # You can have a single directory
  dir: ~
  # or a list of directories for writing logs
  dirs: ~

  # num of messages to accept before forcing a flush
  flush.interval.messages: 1000

  # max time before flushing to disk
  flush.interval.ms: 10000

  # flush.offset.checkpoint.interval.ms: 60000
  # flush.scheduler.interval.ms: 9223372036854775807

  retention.bytes: -1
  # check file eligibility for deletion
  # retention.check.interval.ms: 300000

  # minimum age required for deletion
  retention.hours: 168
  # if set, overrides hours
  # retention.minutes: ~
  # # if set, overrides hours & minutes
  # retention.ms: ~

  # roll.hours: 160
  # roll.jitter.hours: 0
  # roll.jitter.ms: ~

  # # max size of a log segment files 
  # segment.bytes: 1073741824
  # segment.delete.delay.ms: 60000



  cleaner.backoff.ms: 15000
  cleaner.dedupe.buffer.size: 134217728
  cleaner.delete.retention.ms: 86400000
  cleaner.enable: true
  cleaner.io.buffer.load.factor: 0.9
  # total memory used for log cleaning across all cleaner threads
  cleaner.io.buffer.size: 524288
  # Cleanter will be throttled so that the sum of read+write i/o is less than this
  cleaner.io.max.bytes.per.second: ~
  # min ratio of dirty/total logs to trigger a clean
  cleaner.min.cleanable.ratio: 0.5
  cleaner.threads: 1

  # can be delete or compact
  cleanup.policy: delete

  # index.interval.bytes: 4096
  # index.size.max.bytes: 10485760

  # # pre allocate a file before using new segment, set to true on winblows
  # preallocate: false



# Must be format: [type, host, port]
# example:  `ssl://0.0.0.0:9093` or `plaintext://server.mydomain.com:9092`
listeners:
  - 'plaintext://0.0.0.0:9092'

zookeeper:
  chroot: '/kafka'
  connect:
    - 'localhost:2181'
  connection.timeout.ms: 6000
  session.timeout.ms: 6000
  set.acl: false


principal:
  # namespace of the class to use, override this if you really know your stuff
  builder.class: ~

num:
  network.threads: 3
  io.threads: 8

  # default number of partitions
  partitions: 1
  replica.fetchers: 1
  recovery.threads.per.data.dir: 1


socket:
  send.buffer.bytes: 102400
  receive.buffer.bytes: 102400
  request.max.bytes: 104857600

min:
  insync.replicas: 1

offset:
  metadata.max_bytes: 4096
  commit.required.acks: -1
  commit.timeout.ms: 5000
  load.buffer.size: 5242880
  retention.check.interval.ms: 600000
  retention.minutes: 1440
  topic.compression.codec: 0
  topic.num.partitions: 50
  topic.replication.factor: 3
  topic.segment.bytes: 104857600

queued:
  max.requests: 500

quota:
  consumer.default: ~
  producer.default: ~

replica:
  fetch.max.bytes: 1048576
  fetch.min.bytes: 1
  fetch.wait.max.ms: 500
  high.watermark.checkpoint.interval.ms: 5000
  lag.time.max.ms: 10000
  socket.receive.buffer.bytes: 65536
  socket.timeout.ms: 30000
  fetch.backoff.ms: 1000

request:
  timeout.ms: 30000

unclean:
  leader.election.enable: true

connections:
  max.idle.ms: 600000

controlled:
  shutdown.enable: true
  shutdown.max.retries: 3
  shutdown.retry.backoff.ms: 5000

controller:
  socket.timeout.ms: 30000

default:
  replication.factor: 1

fetch:
  purgatory.purge.interval.requests: 1000

group:
  max.session.timeout.ms: 30000
  min.session.timeout.ms: 6000

inter:
  broker.protocol.version: ~

max:
  connections.per.ip: ~
  connections.per.io.overrides: ~

producer:
  purgatory.purge.interval.requests: 1000
{% endload %}
